{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polynomial_regression_with_ridge_regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcfatbeard57/Hands-On-ML-Tensor-FLow/blob/main/polynomial_regression_with_ridge_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCA8Q4BlVaYs"
      },
      "source": [
        "Chapter 4 Training Models: Regularized Linear Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiELhscGT5eT"
      },
      "source": [
        "# Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLKfEq10eFLD"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\r\n",
        "ridge_reg.fit(X, y)\r\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq-h1H7OCpwm"
      },
      "source": [
        "## Using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvp5ucZtCeWp"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "\r\n",
        "model = make_pipeline(PolynomialFeatures(deg_4),Ridge(alpha=0.02,solver='cholesky'))\r\n",
        "model.fit(x_train,t_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkHwrR_EB_0"
      },
      "source": [
        "# another solver can also be used\r\n",
        "solver = 'sag'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLxB_JElCu8C"
      },
      "source": [
        "### Where to do scaling???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrJHkvt3C07K"
      },
      "source": [
        "\r\n",
        "## Using SGD\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC9cKNmdCk5G"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "sgd_reg = SGDRegressor(penalty=\"l2\")\r\n",
        "sgd_reg.fit(X, y.ravel())\r\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ7Tu0K7Et93"
      },
      "source": [
        "### Where to give alpha value here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVpDWq4mT8vo"
      },
      "source": [
        "# Lasso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knnxMTYlCk1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u89DzZfCkxM"
      },
      "source": [
        "from sklearn.linear_model import Lasso\r\n",
        "lasso_reg = Lasso(alpha=0.1)\r\n",
        "lasso_reg.fit(X, y)\r\n",
        "lasso_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXRvCEFXUGcp"
      },
      "source": [
        "### Using SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhOUGfMECktf"
      },
      "source": [
        "from sklearn.linear_model import Ridge\r\n",
        "sgd_reg = SGDRegressor(penalty=\"l1\")\r\n",
        "sgd_reg.fit(X, y.ravel())\r\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rtQ_mkhU6eL"
      },
      "source": [
        "# Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKqk73fU9Q2"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\r\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\r\n",
        "elastic_net.fit(X, y)\r\n",
        "elastic_net.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlM4Db0rVZMw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCevi15U0Ch"
      },
      "source": [
        "# So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjzA3NVlUz-K"
      },
      "source": [
        "Avoid plain Linear Regression. \r\n",
        "Ridge is a good default, \r\n",
        "If only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless featuresâ€™ weights down to zero as we have discussed. \r\n",
        "In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUImYbqVWXQU"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCn21urAWd4R"
      },
      "source": [
        "stop training as soon as the validation error reaches a minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-HkCxNLU4KI"
      },
      "source": [
        "# As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\r\n",
        "# naturally goes down, and so does its prediction error on the validation set. However,\r\n",
        "# after a while the validation error stops decreasing and actually starts to go back up.\r\n",
        "# This indicates that the model has started to overfit the training data. With early stopping\r\n",
        "# you just stop training as soon as the validation error reaches the minimum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2slseLZWqdx"
      },
      "source": [
        "With Stochastic and Mini-batch Gradient Descent, the curves are\r\n",
        "not so smooth, and it may be hard to know whether you have\r\n",
        "reached the minimum or not. One solution is to stop only after the\r\n",
        "validation error has been above the minimum for some time (when\r\n",
        "you are confident that the model will not do any better), then roll\r\n",
        "back the model parameters to the point where the validation error\r\n",
        "was at a minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQDrCxSWniv"
      },
      "source": [
        "from sklearn.base import clone\r\n",
        "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,\r\n",
        "                      learning_rate=\"constant\", eta0=0.0005)\r\n",
        "minimum_val_error = float(\"inf\")\r\n",
        "best_epoch = None\r\n",
        "best_model = None\r\n",
        "\r\n",
        "for epoch in range(1000):\r\n",
        "  sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\r\n",
        "  y_val_predict = sgd_reg.predict(X_val_poly_scaled)\r\n",
        "  val_error = mean_squared_error(y_val_predict, y_val)\r\n",
        "  if val_error < minimum_val_error:\r\n",
        "    minimum_val_error = val_error\r\n",
        "    best_epoch = epoch\r\n",
        "    best_model = clone(sgd_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHHfumetW19I"
      },
      "source": [
        "Note that with warm_start=True, when the fit() method is called, it just continues\r\n",
        "training where it left off instead of restarting from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SssPLQ9peOKd"
      },
      "source": [
        "# Cloning best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN2pkVYcW2x0"
      },
      "source": [
        "\r\n",
        "from sklearn.base import clone\r\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\r\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\r\n",
        "\r\n",
        "minimum_val_error = float(\"inf\")\r\n",
        "best_epoch = None\r\n",
        "best_model = None\r\n",
        "for epoch in range(1000):\r\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\r\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\r\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\r\n",
        "    if val_error < minimum_val_error:\r\n",
        "        minimum_val_error = val_error\r\n",
        "        best_epoch = epoch\r\n",
        "        best_model = clone(sgd_reg)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}